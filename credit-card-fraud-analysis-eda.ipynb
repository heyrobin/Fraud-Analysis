{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<h1><center> Credit Card Fraud Analysis and Prediction ðŸ‘¤ðŸ’°</center></h1>\n    ","metadata":{"execution":{"iopub.status.busy":"2022-02-14T18:24:12.262191Z","iopub.execute_input":"2022-02-14T18:24:12.262585Z","iopub.status.idle":"2022-02-14T18:24:12.291637Z","shell.execute_reply.started":"2022-02-14T18:24:12.262492Z","shell.execute_reply":"2022-02-14T18:24:12.290379Z"}}},{"cell_type":"markdown","source":"<center><img src=\"https://i.ibb.co/9mPwvKZ/1936a278707545.png\" width=\"600\"></center>","metadata":{}},{"cell_type":"markdown","source":"Hello and greeting readers in this notebook we are going to analyze and predict fraudulent transactions from the data. Right now the notebook is on initial phase where I am updating it regularly with my trial and errors of finding best solution to perform the best predictive method to detect transactions.\n\nFollowing are the tasks.\n* Analyzing the data\n* Treating the imbalance data\n* building a model","metadata":{}},{"cell_type":"code","source":"\"libraries\"\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.metrics import confusion_matrix, classification_report","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-02-14T15:31:49.135896Z","iopub.execute_input":"2022-02-14T15:31:49.136290Z","iopub.status.idle":"2022-02-14T15:31:49.638061Z","shell.execute_reply.started":"2022-02-14T15:31:49.136231Z","shell.execute_reply":"2022-02-14T15:31:49.637046Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/creditcardfraud/creditcard.csv')\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-14T15:31:49.643025Z","iopub.execute_input":"2022-02-14T15:31:49.643731Z","iopub.status.idle":"2022-02-14T15:31:53.245403Z","shell.execute_reply.started":"2022-02-14T15:31:49.643677Z","shell.execute_reply":"2022-02-14T15:31:53.244326Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"df.info()","metadata":{"execution":{"iopub.status.busy":"2022-02-14T15:31:53.247222Z","iopub.execute_input":"2022-02-14T15:31:53.247670Z","iopub.status.idle":"2022-02-14T15:31:53.289819Z","shell.execute_reply.started":"2022-02-14T15:31:53.247624Z","shell.execute_reply":"2022-02-14T15:31:53.288537Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"df.describe().T","metadata":{"execution":{"iopub.status.busy":"2022-02-14T15:31:53.293609Z","iopub.execute_input":"2022-02-14T15:31:53.293887Z","iopub.status.idle":"2022-02-14T15:31:53.747375Z","shell.execute_reply.started":"2022-02-14T15:31:53.293854Z","shell.execute_reply":"2022-02-14T15:31:53.746265Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"## Missing Values","metadata":{}},{"cell_type":"code","source":"df.isna().sum().sum()\n\nprint(f'There {df.isna().sum().sum()} Missing Values in the dataset')","metadata":{"execution":{"iopub.status.busy":"2022-02-14T15:31:53.749547Z","iopub.execute_input":"2022-02-14T15:31:53.749915Z","iopub.status.idle":"2022-02-14T15:31:53.801992Z","shell.execute_reply.started":"2022-02-14T15:31:53.749866Z","shell.execute_reply":"2022-02-14T15:31:53.800803Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"we found no missing values from the data which is a good news for further exploration and modeling","metadata":{}},{"cell_type":"markdown","source":"<h1><center> Exploritory Data Analysis </center></h1>","metadata":{}},{"cell_type":"code","source":"\"libraries\"\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"execution":{"iopub.status.busy":"2022-02-14T15:31:53.803674Z","iopub.execute_input":"2022-02-14T15:31:53.804025Z","iopub.status.idle":"2022-02-14T15:31:53.894176Z","shell.execute_reply.started":"2022-02-14T15:31:53.803976Z","shell.execute_reply":"2022-02-14T15:31:53.893220Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"sns.countplot(df[\"Class\"]);","metadata":{"execution":{"iopub.status.busy":"2022-02-14T15:31:53.895546Z","iopub.execute_input":"2022-02-14T15:31:53.895804Z","iopub.status.idle":"2022-02-14T15:31:54.058504Z","shell.execute_reply.started":"2022-02-14T15:31:53.895771Z","shell.execute_reply":"2022-02-14T15:31:54.057243Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"woah, there is very large diffrence between the class 0 and 1. It is an Imbalanced dataset<br>\n\n\n**what is Imbalanced data?**\n\nImbalanced data refers to those types of datasets where the target class has an uneven distribution of observations, i.e one class label has a very high number of observations and the other has a very low number of observations. ","metadata":{}},{"cell_type":"markdown","source":"## Features correlation","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(36,9))\nsns.heatmap(df.corr(),annot=True)","metadata":{"execution":{"iopub.status.busy":"2022-02-14T15:31:54.060478Z","iopub.execute_input":"2022-02-14T15:31:54.061579Z","iopub.status.idle":"2022-02-14T15:32:00.352628Z","shell.execute_reply.started":"2022-02-14T15:31:54.061477Z","shell.execute_reply":"2022-02-14T15:32:00.351682Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"\n\n<h1><center> Modeling </center></h1>","metadata":{}},{"cell_type":"markdown","source":"First we have to blance the data. for that we have to undersample or oversample if we do not have balanced data we will be using the recall and precision otherwise accuracy.","metadata":{}},{"cell_type":"code","source":"\"models\"\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\n\n\"metrics\"\nfrom sklearn.metrics import accuracy_score","metadata":{"execution":{"iopub.status.busy":"2022-02-14T15:32:00.354055Z","iopub.execute_input":"2022-02-14T15:32:00.354503Z","iopub.status.idle":"2022-02-14T15:32:00.387385Z","shell.execute_reply.started":"2022-02-14T15:32:00.354468Z","shell.execute_reply":"2022-02-14T15:32:00.386747Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"X = df.drop(columns='Class', axis=1)\ny = df['Class']\n\nX_train,X_test,y_train,y_test = train_test_split(X,y, test_size=0.2, random_state = 42)","metadata":{"execution":{"iopub.status.busy":"2022-02-14T15:32:00.388818Z","iopub.execute_input":"2022-02-14T15:32:00.389243Z","iopub.status.idle":"2022-02-14T15:32:00.518990Z","shell.execute_reply.started":"2022-02-14T15:32:00.389211Z","shell.execute_reply":"2022-02-14T15:32:00.517813Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"LR = LogisticRegression(solver='liblinear').fit(X_train,y_train)\n\nPreds = LR.predict(X_train)\nLR_AC = accuracy_score(Preds,y_train)\n\nprint(f'Accuracy Score = {LR_AC}')","metadata":{"execution":{"iopub.status.busy":"2022-02-14T15:32:00.520526Z","iopub.execute_input":"2022-02-14T15:32:00.520764Z","iopub.status.idle":"2022-02-14T15:32:05.723946Z","shell.execute_reply.started":"2022-02-14T15:32:00.520736Z","shell.execute_reply":"2022-02-14T15:32:05.723039Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"Preds2 = LR.predict(X_test)\nLR_AC2 = accuracy_score(Preds2,y_test)\n\nprint(f'Accuracy Score = {LR_AC2}')","metadata":{"execution":{"iopub.status.busy":"2022-02-14T15:32:05.725729Z","iopub.execute_input":"2022-02-14T15:32:05.726557Z","iopub.status.idle":"2022-02-14T15:32:05.754247Z","shell.execute_reply.started":"2022-02-14T15:32:05.726491Z","shell.execute_reply":"2022-02-14T15:32:05.753029Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"sns.heatmap(confusion_matrix(Preds2,y_test),annot = True,);","metadata":{"execution":{"iopub.status.busy":"2022-02-14T15:32:05.763189Z","iopub.execute_input":"2022-02-14T15:32:05.766123Z","iopub.status.idle":"2022-02-14T15:32:06.111721Z","shell.execute_reply.started":"2022-02-14T15:32:05.766046Z","shell.execute_reply":"2022-02-14T15:32:06.111077Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"print(classification_report(y_test, Preds2))","metadata":{"execution":{"iopub.status.busy":"2022-02-14T15:32:06.112806Z","iopub.execute_input":"2022-02-14T15:32:06.113147Z","iopub.status.idle":"2022-02-14T15:32:06.178573Z","shell.execute_reply.started":"2022-02-14T15:32:06.113117Z","shell.execute_reply":"2022-02-14T15:32:06.177918Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"### Undersampling Technique\n\nUndersampling is a technique to balance uneven datasets by keeping all of the data in the minority class and decreasing the size of the majority class. It is one of several techniques data scientists can use to extract more accurate information from originally imbalanced datasets.","metadata":{}},{"cell_type":"code","source":"legit = df[df[\"Class\"] == 0]\nfraud = df[df[\"Class\"] == 1]\n\n\nprint(f'Legit: {fraud.Class.value_counts()[1]} & Fraud : {legit.Class.value_counts()[0]}')","metadata":{"execution":{"iopub.status.busy":"2022-02-14T15:32:06.179697Z","iopub.execute_input":"2022-02-14T15:32:06.180303Z","iopub.status.idle":"2022-02-14T15:32:06.237060Z","shell.execute_reply.started":"2022-02-14T15:32:06.180268Z","shell.execute_reply":"2022-02-14T15:32:06.236135Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"ls = legit.sample(n=492)\nls.Class.value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-02-14T15:32:06.238667Z","iopub.execute_input":"2022-02-14T15:32:06.239115Z","iopub.status.idle":"2022-02-14T15:32:06.257384Z","shell.execute_reply.started":"2022-02-14T15:32:06.239058Z","shell.execute_reply":"2022-02-14T15:32:06.256190Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"df2 = pd.concat([fraud,ls], axis = 0)","metadata":{"execution":{"iopub.status.busy":"2022-02-14T15:32:06.259082Z","iopub.execute_input":"2022-02-14T15:32:06.259338Z","iopub.status.idle":"2022-02-14T15:32:06.271332Z","shell.execute_reply.started":"2022-02-14T15:32:06.259307Z","shell.execute_reply":"2022-02-14T15:32:06.270504Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"sns.countplot(df2[\"Class\"]);","metadata":{"execution":{"iopub.status.busy":"2022-02-14T15:32:06.272824Z","iopub.execute_input":"2022-02-14T15:32:06.273409Z","iopub.status.idle":"2022-02-14T15:32:06.456994Z","shell.execute_reply.started":"2022-02-14T15:32:06.273361Z","shell.execute_reply":"2022-02-14T15:32:06.456315Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"The class has been balanced by equal values. Lets perform modeling on this new balanced data","metadata":{}},{"cell_type":"code","source":"X = df2.drop(columns='Class', axis=1)\ny = df2['Class']","metadata":{"execution":{"iopub.status.busy":"2022-02-14T15:32:06.458819Z","iopub.execute_input":"2022-02-14T15:32:06.459146Z","iopub.status.idle":"2022-02-14T15:32:06.465948Z","shell.execute_reply.started":"2022-02-14T15:32:06.459089Z","shell.execute_reply":"2022-02-14T15:32:06.465278Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"X_train,X_test,y_train,y_test = train_test_split(X,y, test_size=0.2, random_state = 42)","metadata":{"execution":{"iopub.status.busy":"2022-02-14T15:32:06.467173Z","iopub.execute_input":"2022-02-14T15:32:06.467987Z","iopub.status.idle":"2022-02-14T15:32:06.486536Z","shell.execute_reply.started":"2022-02-14T15:32:06.467952Z","shell.execute_reply":"2022-02-14T15:32:06.485754Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"model = LogisticRegression(solver='liblinear').fit(X_train,y_train)","metadata":{"execution":{"iopub.status.busy":"2022-02-14T15:32:06.487828Z","iopub.execute_input":"2022-02-14T15:32:06.488170Z","iopub.status.idle":"2022-02-14T15:32:06.509187Z","shell.execute_reply.started":"2022-02-14T15:32:06.488141Z","shell.execute_reply":"2022-02-14T15:32:06.507895Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"pred = model.predict(X_train)\nactr = accuracy_score(pred,y_train)\nprint(f'Accuracy Score = {actr}')","metadata":{"execution":{"iopub.status.busy":"2022-02-14T15:32:06.510918Z","iopub.execute_input":"2022-02-14T15:32:06.511691Z","iopub.status.idle":"2022-02-14T15:32:06.529749Z","shell.execute_reply.started":"2022-02-14T15:32:06.511638Z","shell.execute_reply":"2022-02-14T15:32:06.528642Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"pred = model.predict(X_test)\nacts = accuracy_score(pred,y_test)\nerror_rate = 1 - acts\nprint(f'Accuracy Score = {acts}')\nprint(f'error_rate = {error_rate}')","metadata":{"execution":{"iopub.status.busy":"2022-02-14T15:32:06.534934Z","iopub.execute_input":"2022-02-14T15:32:06.538908Z","iopub.status.idle":"2022-02-14T15:32:06.552616Z","shell.execute_reply.started":"2022-02-14T15:32:06.538820Z","shell.execute_reply":"2022-02-14T15:32:06.551453Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":"There is no overfitting but is the model accurate enough to predict the fraud transaction?","metadata":{}},{"cell_type":"code","source":"sns.heatmap(confusion_matrix(y_test,pred),\n                annot = True,\n                );","metadata":{"execution":{"iopub.status.busy":"2022-02-14T15:32:06.554769Z","iopub.execute_input":"2022-02-14T15:32:06.555423Z","iopub.status.idle":"2022-02-14T15:32:06.778006Z","shell.execute_reply.started":"2022-02-14T15:32:06.555371Z","shell.execute_reply":"2022-02-14T15:32:06.776998Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"markdown","source":"<h1><center>To be continued...</center>","metadata":{}}]}